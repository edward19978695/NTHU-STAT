# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Zt-xLxg74nyHrgykND3Ad91Q-wHqb0o
"""

import numpy as np
import torch
import h5py
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from sklearn.preprocessing import OneHotEncoder
# Import necessary packages.
import os
import torch.nn as nn
# "ConcatDataset" and "Subset" are possibly useful when doing semi-supervised learning.
from torch.utils.data import DataLoader, Dataset
# from torchvision.datasets import DatasetFolder, VisionDataset

# This is for the progress bar.
from tqdm.auto import tqdm

# Load training data
# hf_train = h5py.File('/content/drive/MyDrive/DeepLearning/Homework/HW3/Dataset/Signs_Data_Training.h5', 'r')
hf_train = h5py.File('Dataset/Signs_Data_Training.h5', 'r')
train_image = np.array(hf_train["train_set_x"])
train_label = np.array(hf_train["train_set_y"])

# Data pre-process
onehotencoder = OneHotEncoder()
train_label = onehotencoder.fit_transform(train_label.reshape(-1, 1)).toarray()
train_image = torch.from_numpy(train_image).permute(0,3,1,2)
train_image = 2*(train_image/255) - 1

# 5-fold CV
cv_5_fold = KFold(n_splits=5, shuffle=True, random_state=1)
train_image_cv = dict() ; train_label_cv = dict()
val_image_cv = dict() ; val_label_cv = dict()
for i, (train_index, val_index) in enumerate(cv_5_fold.split(train_image,train_label)):
  key = "f"+str(i+1)
  train_image_cv[key] = train_image[train_index]
  train_label_cv[key] = train_label[train_index]
  val_image_cv[key] = train_image[val_index]
  val_label_cv[key] = train_label[val_index]

# Accuracy function
def cal_accuracy(prediction, label):
    ''' Calculate Accuracy, please don't modify this part
        Args:
            prediction (with dimension N): Predicted Value
            label  (with dimension N): Label
        Returns:
            accuracy:　Accuracy
    '''

    accuracy = 0
    number_of_data = len(prediction)
    for i in range(number_of_data):
        accuracy += float(prediction[i] == label[i])
    accuracy = (accuracy / number_of_data) * 100

    return accuracy

## wrap train dataset
class Train_Loader(Dataset):
    def __init__(self, img_arr, label_arr):
        '''
            define img_arr, label_arr
        '''
        self.img_arr = img_arr
        self.label_arr = label_arr

    def __len__(self):
        '''
            return length
        '''
        return len(self.img_arr)

    def __getitem__(self, index):
        '''
            process each img and label
        '''
        img = self.img_arr[index]
        label = self.label_arr[index]
        return img,label

# Classifier model
class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)
        # input 維度 [3, 64, 64]
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),  # [64, 64, 64]
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]

            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]

            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),      # [256, 8, 8]

            nn.Conv2d(256, 512, 3, 1, 1), # [512, 8, 8]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [512, 4, 4]

            nn.Conv2d(512, 1024, 3, 1, 1), # [1024, 4, 4]
            nn.BatchNorm2d(1024),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [1024, 2, 2]

            nn.Conv2d(1024, 2048, 3, 1, 1), # [2048, 2, 2]
            nn.BatchNorm2d(2048),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),       # [2048, 1, 1]
        )
        self.fc = nn.Sequential(
            nn.Linear(2048*1*1, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 6)
        )

    def forward(self, x):
        out = self.cnn(x)
        out = out.contiguous().view(out.size()[0], -1)
        return self.fc(out)

# "cuda" only when GPUs are available.
device = "cuda" if torch.cuda.is_available() else "cpu"

# The number of training epochs and patience.
n_epochs = 50

# For the classification task, we use cross-entropy as the measurement of performance.
criterion = nn.CrossEntropyLoss()

batch_size = 128
train_loss_cv = np.empty((5,n_epochs))
train_acc_cv = np.empty((5,n_epochs))
val_loss_cv = np.empty((5,n_epochs))
val_acc_cv = np.empty((5,n_epochs))

# Train 5-fold CV models
for k in range(5):
  key = "f" + str(k+1)
  train_set = Train_Loader(train_image_cv[key], train_label_cv[key])
  train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
  val_set = Train_Loader(val_image_cv[key], val_label_cv[key])
  val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)

  # Initialize a model, and put it on the device specified.
  model = Classifier().to(device)

  # Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.
  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) 

  print("Fold", k+1)

  for epoch in range(n_epochs):
    # ---------- Training ----------
    # Make sure the model is in train mode before training.
    model.train()

    # These are used to record information in training.
    train_loss = []
    train_accs = []
    for batch in tqdm(train_loader):

        # A batch consists of image data and corresponding labels.
        imgs, labels = batch

        # Forward the data. (Make sure data and model are on the same device.)
        logits = model(imgs.to(device))

        # Calculate the cross-entropy loss.
        loss = criterion(logits, labels.to(device))

        # Gradients stored in the parameters in the previous step should be cleared out first.
        optimizer.zero_grad()

        # Compute the gradients for parameters.
        loss.backward()

        # Clip the gradient norms for stable training.
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

        # Update the parameters with computed gradients.
        optimizer.step()

        # Compute the accuracy for current batch.
        acc = cal_accuracy(np.argmax(logits.cpu().data.numpy(), axis=-1),np.argmax(labels.cpu(), axis=-1))

        # Record the loss and accuracy.
        train_loss.append(loss.item())
        train_accs.append(acc)

    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)

    train_loss_cv[k,epoch] = train_loss
    train_acc_cv[k,epoch] = train_acc

    # Print the information.
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # ---------- Validation ----------
    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.
    model.eval()

    # These are used to record information in validation.
    val_loss = []
    val_accs = []

    # Iterate the validation set by batches.
    for batch in tqdm(val_loader):

        # A batch consists of image data and corresponding labels.
        imgs, labels = batch

        # We don't need gradient in validation.
        # Using torch.no_grad() accelerates the forward process.
        with torch.no_grad():
            logits = model(imgs.to(device))

        # We can still compute the loss (but not the gradient).
        loss = criterion(logits, labels.to(device))

        # Compute the accuracy for current batch.
        acc = cal_accuracy(np.argmax(logits.cpu().data.numpy(), axis=-1),np.argmax(labels.cpu(),axis=-1))

        # Record the loss and accuracy.
        val_loss.append(loss.item())
        val_accs.append(acc)
        #break

    # The average loss and accuracy for entire validation set is the average of the recorded values.
    val_loss = sum(val_loss) / len(val_loss)
    val_acc = sum(val_accs) / len(val_accs)

    val_loss_cv[k,epoch] = val_loss
    val_acc_cv[k,epoch] = val_acc

    # Print the information.
    print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {val_loss:.5f}, acc = {val_acc:.5f}")

# Visualization
fig, ax = plt.subplots(2, 3, sharey=True, sharex = True)
ax[0,0].plot(range(1,n_epochs+1),train_loss_cv[0,:], color="blue")
ax[0,0].plot(range(1,n_epochs+1), val_loss_cv[0,:], color="red")
ax[0,0].set_title("Fold 1")
ax[0,1].plot(range(1,n_epochs+1),train_loss_cv[1,:], color="blue")
ax[0,1].plot(range(1,n_epochs+1), val_loss_cv[1,:], color="red")
ax[0,1].set_title("Fold 2")
ax[0,2].plot(range(1,n_epochs+1),train_loss_cv[2,:], color="blue")
ax[0,2].plot(range(1,n_epochs+1), val_loss_cv[2,:], color="red")
ax[0,2].set_title("Fold 3")
ax[1,0].plot(range(1,n_epochs+1),train_loss_cv[3,:], color="blue")
ax[1,0].plot(range(1,n_epochs+1), val_loss_cv[3,:], color="red")
ax[1,0].set_title("Fold 4")
ax[1,1].plot(range(1,n_epochs+1),train_loss_cv[4,:], color="blue")
ax[1,1].plot(range(1,n_epochs+1), val_loss_cv[4,:], color="red")
ax[1,1].set_title("Fold 5")
ax[1,2].plot(range(1,n_epochs+1),np.mean(train_loss_cv, axis=0), color="blue")
ax[1,2].plot(range(1,n_epochs+1), np.mean(val_loss_cv, axis=0), color="red")
ax[1,2].set_title("Avg. of 5 Folds")
plt.savefig("5fold_CV_train_loss.png", dpi = 300)
plt.show()

fig, ax = plt.subplots(2, 3, sharey=True, sharex = True)
ax[0,0].plot(range(1,n_epochs+1),train_acc_cv[0,:], color="blue")
ax[0,0].plot(range(1,n_epochs+1), val_acc_cv[0,:], color="red")
ax[0,0].set_title("Fold 1")
ax[0,1].plot(range(1,n_epochs+1),train_acc_cv[1,:], color="blue")
ax[0,1].plot(range(1,n_epochs+1), val_acc_cv[1,:], color="red")
ax[0,1].set_title("Fold 2")
ax[0,2].plot(range(1,n_epochs+1),train_acc_cv[2,:], color="blue")
ax[0,2].plot(range(1,n_epochs+1), val_acc_cv[2,:], color="red")
ax[0,2].set_title("Fold 3")
ax[1,0].plot(range(1,n_epochs+1),train_acc_cv[3,:], color="blue")
ax[1,0].plot(range(1,n_epochs+1), val_acc_cv[3,:], color="red")
ax[1,0].set_title("Fold 4")
ax[1,1].plot(range(1,n_epochs+1),train_acc_cv[4,:], color="blue")
ax[1,1].plot(range(1,n_epochs+1), val_acc_cv[4,:], color="red")
ax[1,1].set_title("Fold 5")
ax[1,2].plot(range(1,n_epochs+1),np.mean(train_acc_cv, axis=0), color="blue")
ax[1,2].plot(range(1,n_epochs+1), np.mean(val_acc_cv, axis=0), color="red")
ax[1,2].set_title("Avg. of 5 Folds")
plt.savefig("5fold_CV_train_acc.png", dpi = 300)
plt.show()

# Train final model
train_set = Train_Loader(train_image, train_label)
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
# Initialize a model, and put it on the device specified.
model = Classifier().to(device)

# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)

for epoch in range(n_epochs):
    # ---------- Training ----------
    # Make sure the model is in train mode before training.
    model.train()

    # These are used to record information in training.
    train_loss = []
    train_accs = []
    for batch in tqdm(train_loader):

        # A batch consists of image data and corresponding labels.
        imgs, labels = batch

        # Forward the data. (Make sure data and model are on the same device.)
        logits = model(imgs.to(device))

        # Calculate the cross-entropy loss.
        loss = criterion(logits, labels.to(device))

        # Gradients stored in the parameters in the previous step should be cleared out first.
        optimizer.zero_grad()

        # Compute the gradients for parameters.
        loss.backward()

        # Clip the gradient norms for stable training.
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)

        # Update the parameters with computed gradients.
        optimizer.step()

        # Compute the accuracy for current batch.
        acc = cal_accuracy(np.argmax(logits.cpu().data.numpy(), axis=-1),np.argmax(labels.cpu(), axis=-1))

        # Record the loss and accuracy.
        train_loss.append(loss.item())
        train_accs.append(acc)

    train_loss = sum(train_loss) / len(train_loss)
    train_acc = sum(train_accs) / len(train_accs)

    # Print the information.
    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")

    # save models
    if epoch == n_epochs-1:
      torch.save(model, "model_weights.pth") # save last model